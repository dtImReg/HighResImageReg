{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rigid Registration\n",
    "\n",
    "This note book contains the code/algorithm required to perform the rigid registration.\n",
    "Note: Initially aretefacts are removed from both image using a blob detector. The blob detector is not very sophisticated and is based on the assumption that the biggest blob in the pictures provided is the tissue (both for the MALDI and the HandE image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "%matplotlib inline\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import fixed as fxd\n",
    "import sys\n",
    "import os\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Change the flags below to your preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = ''\n",
    "FIXED_IMAGE_FILE_PATH = '' #Enter the MALDI optical image file path\n",
    "MOVING_IMAGE_FILE_PATH = '' #Enter the HandE image file path\n",
    "DOWNSAMPLE = True # Chnage this to false if you wish to attempt regisration without downsampling\n",
    "FACTOR = 0.2\n",
    "FEATURE_DETECTOR = 'AKAZE'\n",
    "MIN_MATCH_COUNT = 10 # Choose the number minimum feature matches required\n",
    "\n",
    "\n",
    "if not os.path.exists(FOLDER_PATH + 'RegisteredImages'):\n",
    "    os.makedirs(FOLDER_PATH + 'RegisteredImages')\n",
    "    FOLDER_PATHNEW = FOLDER_PATH + 'RegisteredImages/'\n",
    "    print FOLDER_PATHNEW\n",
    "else:\n",
    "    FOLDER_PATHNEW = FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movingImage = cv2.imread(MOVING_IMAGE_FILE_PATH) # Here we read in the images, the file location given by above.\n",
    "fixedImage = cv2.imread(FIXED_IMAGE_FILE_PATH )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing code:\n",
    "The following code is a set of methods to clean up the provided images. Note that these steps are specific to the HandE and MALDI images of this project. As both of these images are bi-modal images, a very simple and straightforward blob detection approach is used to isolate and extract the area of the tissue. \n",
    "\n",
    "(Please see the tools.py file for each of the function that is called here)\n",
    "\n",
    "\n",
    "Note: The functions described below should be used in the given order: detectTissue()->extractTissu()->boundRectTissue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(detectTissue()): First a threshold is applied to the image - that is all values of pixels above a certain threshold are sent to 255. We then apply morphological transformations (open and close) which will get rid of smaller blobs (noise) as well as holes in the bigger blobs. The output of this is then used as a mask to produce a binary image. This binary image should contain a mask around the whole of this tissue (the subject of the image that we are interested in) as well as other significantly big artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mIBlobs = tools.detectTissue(img = movingImage, threshold = 200 , kernelSize = 41 , erosionIteration = 2) \n",
    "fIBlobs = tools.detectTissue(img = fixedImage, threshold = 200 , kernelSize = 41 , erosionIteration = 2)\n",
    "plt.subplot(121), plt.imshow(mIBlobs)\n",
    "plt.subplot(122), plt.imshow(fIBlobs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(extractTissue()) We can then use the binary image from above to extract the tissue. We first compute the contours around the blobs, and then identify the one with the biggest contour area - this of course must be the tissue. This is then used to cut out just the tissue from the original image and place it on a white background (you can specify the background color if you wish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movingImageCleaned,mIContours,mIIndex =  tools.extractTissue (img = movingImage,blobs = mIBlobs, backgroundColour = (255,255,255))\n",
    "fixedImageCleaned,fIContours,fIIndex =  tools.extractTissue (img = fixedImage,blobs = fIBlobs, backgroundColour = (255,255,255))\n",
    "\n",
    "plt.subplot(121), plt.imshow(movingImageCleaned)\n",
    "plt.subplot(122), plt.imshow(fixedImageCleaned)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(boundRectTissue()) Here we simply draw the bounding rectangle around the tissue, and extract only what is inside this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movingBRectImage,x1,y1,w1,h1 = tools.boundRectTissue(img = movingImageCleaned, contours= mIContours, idx = mIIndex)\n",
    "fixedBRectImage,x2,y2,w2,h2 = tools.boundRectTissue(img = fixedImageCleaned, contours= fIContours, idx = fIIndex)\n",
    "\n",
    "plt.subplot(121), plt.imshow(movingBRectImage)\n",
    "plt.subplot(122), plt.imshow(fixedBRectImage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registration Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the Project Report for further details of the theory of feature detectors and feature matching. Breifly the code follows these steps:\n",
    "- We do further process the images for feature detection by applying a blur/denoising. This is done by the preProc() and preProcLowRes() functions. In the case we opted to register a lower resolution version of the images we also down sample the images using the downSample() function.\n",
    "- a feature detection algorithm is used to find features in both images. This is done first using the cv2.AKAZE_create() or cv2.ORB_create() to initiate the feature detector object. In principle you can use any other feature detector available in Open CV.\n",
    "- Once this is done detectAndCompute() will find and return the features (also called key points) and the feature detectors. \n",
    "- We then find the matches by \"brute force\", that is we consider all posible matches between feature descriptors and match those who are the closest neighbours. We use cv2.BFMatcher() and knnMatch() to do this.\n",
    "- The matches array will have a lot of false matches, which we atttempt to elminate using the ratio test. \n",
    "- We then estimate a partial affine transformation (technically should be called a similarity transformation), which uses the RANSAC iterative method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-proc complete. Completed histo equalization and blurring\n",
      "Pre-proc complete. Completed histo equalization and blurring\n"
     ]
    }
   ],
   "source": [
    "mImageGray = cv2.cvtColor(movingBRectImage, cv2.COLOR_BGR2GRAY)\n",
    "fImageGray = cv2.cvtColor(fixedBRectImage, cv2.COLOR_BGR2GRAY)\n",
    "if DOWNSAMPLE == True :\n",
    "    \n",
    "    fImageDS =  tools.downSample(fixedImage,FACTOR)\n",
    "    mImageDS =  tools.downSample(movingImage,FACTOR)    \n",
    "    \n",
    "    \n",
    "    fImageGrayDS = tools.downSample(fImageGray,FACTOR) #downsampled fixed image bounded rectangle\n",
    "    mImageGrayDS = tools.downSample(mImageGray,FACTOR) #downsampled img1\n",
    "    \n",
    "    mImageGrayHisto = tools.preProc(mImageGrayDS,11,50)\n",
    "    fImageGrayHisto = tools.preProc(fImageGrayDS,11,50)\n",
    "    \n",
    "    \n",
    "    mImageGrayHisto = tools.regImageReposition( mImageGrayHisto,  mImageDS, (int(x1*FACTOR),int(y1*FACTOR),int(w1*FACTOR),int(h1*FACTOR)) , True)\n",
    "    fImageGrayHisto = tools.regImageReposition( fImageGrayHisto,  fImageDS, (int(x2*FACTOR),int(y2*FACTOR),int(w2*FACTOR),int(h2*FACTOR)), True)\n",
    "    \n",
    "else:\n",
    "    mImageGrayHisto = tools.preProc(mImageGray,73,300)\n",
    "    fImageGrayHisto = tools.preProc(fImageGray,73,300)\n",
    "    \n",
    "    mImageGrayHisto = tools.regImageReposition( mImageGrayHisto,  movingImage, (x1,y1,w1,h1) ,  True)\n",
    "    fImageGrayHisto = tools.regImageReposition( fImageGrayHisto,  fixedImage, (x2,y2,w2,h2) , True)\n",
    "    \n",
    "RERUN = True     # This is a varaible that will ensure that the image varaibles are re-initated in case there has been changes in the preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed  detection...\n"
     ]
    }
   ],
   "source": [
    "featureDetector = None\n",
    "# Initiate feature detector:\n",
    "if FEATURE_DETECTOR == 'AKAZE':\n",
    "    featureDetector = cv2.AKAZE_create()\n",
    "elif FEATURE_DETECTOR == 'ORB':\n",
    "    featureDetector = cv2.ORB_create()\n",
    "\n",
    "# find the keypoints and descriptors with feature detector\n",
    "kp1, des1 = featureDetector.detectAndCompute(mImageGrayHisto,None) # finding the kp and des of the HandE image\n",
    "kp2, des2 = featureDetector.detectAndCompute(fImageGrayHisto,None) # finding the kp and des of the MALDI image\n",
    "print 'Completed  detection...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Key Point Matching...\n"
     ]
    }
   ],
   "source": [
    "if FEATURE_DETECTOR == 'AKAZE' or FEATURE_DETECTOR == 'ORB':\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING) # Here we initiate the Brute force method object - this will essentially iterate through all possible matches hence brute force\n",
    "    matches = bf.knnMatch(des1,des2,k=2) # Finds the K nearest neighbours using the fecture descriptors\n",
    "print 'Completed Key Point Matching...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enough matches found\n",
      "Matches: 25\n",
      "Down Sampled transform matrix:\n",
      "[[  7.77292492e-01   2.49900938e-02   2.03018965e+02]\n",
      " [ -2.49900938e-02   7.77292492e-01   1.82738482e+02]]\n",
      "Final transform matrix:\n",
      "[[  7.77292492e-01   2.49900938e-02   1.01509483e+03]\n",
      " [ -2.49900938e-02   7.77292492e-01   9.13692410e+02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dt716997\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "ratioTestArray = [] # This array will contain all the refined matches, as the bf matcher output will contain some flase matches\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance: # Here we simply use the ratio test as suggested by (D.Lowe, 2004)\n",
    "        ratioTestArray.append(m) # If the ratio between the \n",
    "\n",
    "if len(ratioTestArray)>MIN_MATCH_COUNT:\n",
    "    print \"Enough matches found\"\n",
    "    print \"Matches:\",len(ratioTestArray)\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in ratioTestArray ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in ratioTestArray ]).reshape(-1,1,2)\n",
    "    \n",
    "    M1, mask= cv2.estimateAffinePartial2D(src_pts,dst_pts,method=cv2.RANSAC,ransacReprojThreshold= 5.0, maxIters = 2000)\n",
    "    matchesMask1 = mask.ravel().tolist()\n",
    "\n",
    "    \n",
    "    if DOWNSAMPLE == True and M1 != None:\n",
    "        w,h,_ = fixedImage.shape\n",
    "        \n",
    "        wDS = int(w * FACTOR)\n",
    "        hDS = int(h * FACTOR)\n",
    "        \n",
    "\n",
    "        regImageDS = cv2.warpAffine(mImageDS, M1, (hDS,wDS),borderMode=cv2.BORDER_CONSTANT,borderValue=(255,255,255))\n",
    "        \n",
    "        print 'Down Sampled transform matrix:'\n",
    "        print M1\n",
    "        \n",
    "        M2 = M1\n",
    "        M2[:,2] = M2[:,2]*(1/FACTOR)\n",
    "        regImage = cv2.warpAffine(movingImage, M2, (h,w),borderMode=cv2.BORDER_CONSTANT,borderValue=(255,255,255))\n",
    "\n",
    "        \n",
    "        print 'Final transform matrix:'\n",
    "        print M2\n",
    "        \n",
    "    elif DOWNSAMPLE == False and M1 != None:\n",
    "        w,h = fImageGrayHisto.shape\n",
    "        regImage = cv2.warpAffine(movingImage, M1, (h,w),borderMode=cv2.BORDER_CONSTANT,borderValue=(255,255,255))\n",
    "        \n",
    "        print 'No downsample estimated.Final transform matrix:'\n",
    "        print M1\n",
    "        \n",
    "    elif M1 == None:\n",
    "        print 'No transform found, check matching is correct'\n",
    "        \n",
    "else:\n",
    "    print \"Not enough matches are found - %d/%d\" % (len(ratioTestArray),MIN_MATCH_COUNT)\n",
    "    matchesMask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output : \n",
    "This will show the overlay of the two images, can be used to evaluate the registration visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if RERUN == True: # We preprocess the images so that it shows better when presented as an overaly.\n",
    "    #fImageGrayHisto = cv2.cvtColor(fImageGrayHisto, cv2.COLOR_GRAY2BGR)\n",
    "    if DOWNSAMPLE == True :\n",
    "        fImageGrayHistoUS = tools.preProc(fImageGray,73,75)\n",
    "        fImageGrayHistoUS = tools.regImageReposition( fImageGrayHistoUS,  fixedImage, (x2,y2,w2,h2) , True)\n",
    "        fImageGrayHistoUS = cv2.cvtColor(fImageGrayHistoUS, cv2.COLOR_GRAY2BGR)\n",
    "RERUN = False\n",
    "\n",
    "\n",
    "\n",
    "if DOWNSAMPLE == True :\n",
    "    \n",
    "    print fImageGrayHisto.shape\n",
    "    print regImageDS.shape\n",
    "    \n",
    "    dDestDS = cv2.addWeighted(regImageDS,0.5,fImageGrayHisto,0.5,gamma = 0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dDestUS = cv2.addWeighted(regImage,0.5,fImageGrayHistoUS,0.5,gamma = 0)\n",
    "    \n",
    "    \n",
    "    plt.subplot(121), plt.imshow(dDestDS)\n",
    "    plt.subplot(122), plt.imshow(dDestUS)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    dDest2 = cv2.addWeighted(regImage,0.5,fImageGrayHisto,0.5,gamma = 0)\n",
    "    plt.imshow(dDest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if DOWNSAMPLE == True :\n",
    "    cv2.imwrite(FOLDER_PATHNEW + 'overlayLowRes.jpg',dDestDS)\n",
    "    cv2.imwrite(FOLDER_PATHNEW + 'overlayHighRes.png',dDestUS)\n",
    "    cv2.imwrite(FOLDER_PATHNEW + 'regImageDS.jpg',regImageDS)\n",
    "    cv2.imwrite(FOLDER_PATHNEW + 'regImageUS.jpg',regImage)\n",
    "else:\n",
    "    cv2.imwrite(FOLDER_PATHNEW + 'rigRegImage.jpg',regImage)\n",
    "    cv2.imwrite(FOLDER_PATHNEW + 'rigRegoverlay.jpg',dDest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing(fixedImage,HandE):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    img3__ = plt.imshow(fImageGrayHisto, alpha=fixedImage)\n",
    "    img3__ = plt.imshow(regImageDS, alpha=HandE)\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(testing,fixedImage = (0,1,0.2) ,HandE = (0,1,0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can further analyse:\n",
    "    - the images used to detect features (histogram equalized and blur filter)\n",
    "    - the macthes prior and post RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if DOWNSAMPLE == True :\n",
    "    \n",
    "    \n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                       singlePointColor = None,\n",
    "                       matchesMask = matchesMask1, # draw only inliers\n",
    "                       flags = 2)\n",
    "\n",
    "    img3 = cv2.drawMatches(mImageDS,kp1,fImageDS,kp2,ratioTestArray,None,**draw_params)\n",
    "\n",
    "\n",
    "    draw_params1 = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                       singlePointColor = None,\n",
    "                       matchesMask = None, # draw only inliers\n",
    "                       flags = 2)\n",
    "\n",
    "    img4 = cv2.drawMatches(mImageDS,kp1,fImageDS,kp2,ratioTestArray,None,**draw_params1)\n",
    "\n",
    "    \n",
    "else:    \n",
    "\n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                       singlePointColor = None,\n",
    "                       matchesMask = matchesMask1, # draw only inliers\n",
    "                       flags = 2)\n",
    "\n",
    "    img3 = cv2.drawMatches(movingImage,kp1,fixedImage,kp2,ratioTestArray,None,**draw_params)\n",
    "\n",
    "\n",
    "    draw_params1 = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                       singlePointColor = None,\n",
    "                       matchesMask = None, # draw only inliers\n",
    "                       flags = 2)\n",
    "\n",
    "    img4 = cv2.drawMatches(movingImage,kp1,fixedImage,kp2,ratioTestArray,None,**draw_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(FOLDER_PATHNEW + 'Matches_RANSACMask.jpg',img3)\n",
    "cv2.imwrite(FOLDER_PATHNEW + 'Matches_NoMask.jpg',img4)\n",
    "cv2.imwrite(FOLDER_PATHNEW + 'histoimagesMI.jpg',mImageGrayHisto)\n",
    "cv2.imwrite(FOLDER_PATHNEW + 'histoimagesFI.jpg',fImageGrayHisto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT !\n",
    "Please run the cell below to generate the histo-equlized image of the registered image. This is the image you will be using as the moving image when doing the deformable registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-proc complete. Completed histo equalization and blurring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DOWNSAMPLE == True :\n",
    "    mImageGrayHisto = tools.preProc(mImageGray,73,75)\n",
    "    mImageGrayHisto = tools.regImageReposition( mImageGrayHisto,  movingImage, (x1,y1,w1,h1) ,  True)\n",
    "    regImageHisto = cv2.warpAffine(mImageGrayHisto, M2, (h,w),borderMode=cv2.BORDER_CONSTANT,borderValue=(255,255,255))\n",
    "else:\n",
    "    regImageHisto = cv2.warpAffine(mImageGrayHisto, M2, (h,w),borderMode=cv2.BORDER_CONSTANT,borderValue=(255,255,255))\n",
    "\n",
    "cv2.imwrite(FOLDER_PATHNEW + 'regImageHisto.jpg',regImageHisto)\n",
    "cv2.imwrite(FOLDER_PATHNEW + 'mImageHistoUS.jpg',fImageGrayHistoUS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
